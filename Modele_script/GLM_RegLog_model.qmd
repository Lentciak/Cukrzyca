---
title: "GLM_RegLog"
author: "Szymon Olędzki, Igor Nowiński"
format: html
theme: 
  dark: darkly
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r}
library(tidyverse)
library(caret)
library(doSNOW)
```

```{r}
#Załadowanie zbioru danych
dane <- read.csv("diabetes_binary.csv", header = TRUE, sep = ",", dec = ".")
```

```{r}
#Podział zbioru na zmienne jakościowe i ilościowe, 
#dodatkowo utworzenie podzbioru wyłącznie ze zmiennymi jakościowymi

numeric_vars <- c('BMI', 'MentHlth', 'PhysHlth')

num_vars_cols <- which(colnames(dane) %in% numeric_vars)

dane <- dane %>%
  mutate(across(!all_of(num_vars_cols), as.factor))

#Należy dodatkowo wyróżnić zmienne, w których ważny jest porządek danych wartości nominalnych, jak np. wiek. Z tego powodu musimy je dodatkowo uporządkować.

with_order_vars <- c('Age', 'Education', 'Income', 'GenHlth')

dane <- dane %>%
  mutate(across(all_of(with_order_vars), as.ordered))

dane$GenHlth <- fct_rev(dane$GenHlth)

#Pozbywamy się również duplikatów danych ze zbioru
dane <- distinct(dane)
```

```{r}
#Usuwamy ze zbioru zmienne o niskiej wariancji, bo nie wniosą do modelu istotnych informacji
dane <- dane[,-nearZeroVar(dane)]
```

```{r}
#Łączymy pierwsze 3 poziomy zmiennej 'Education', ze względu na bardzo niewielką ilość informacji jaką przekazują osobno

dane$Education <- fct_collapse(dane$Education, '3-' = c('1', '2', '3'))
```

```{r}
#Dodatkowo zmieniamy poziomy naszej zmiennej objaśniającej na 'yes'(1) i 'no'(0) (konieczne dla poprawnego działania RandomForest i XGBoost)
levels(dane$Diabetes_binary) <- c('no', 'yes')
```

```{r}
#PODZIAŁ DANYCH
set.seed(2024)

trainIndex <- createDataPartition(dane$Diabetes_binary,
                                  p = .75,
                                  list = FALSE)

dane.train <- dane[trainIndex,]
dane.test <- dane[-trainIndex,]
```

```{r}
# PREPROCESSING TRENING

# Przekształcamy zmienne kategoryczne
dummyModel <- dummyVars(Diabetes_binary ~ ., data = dane.train, fullRank = T)
dane.train.X <- as.data.frame(predict(dummyModel, dane.train))

# Przekształcamy zmienne numeryczne na zakres 0-1
rangeModel <- preProcess(dane.train.X, method = 'range')
dane.train.X <- predict(rangeModel, dane.train.X)

# Łączymy zmienne zależną i niezależne w zbiorze treningowym
dane.train <- cbind(Diabetes_binary = dane.train$Diabetes_binary, dane.train.X)

# PREPROCESSING TEST

# Przekształcamy zmienne kategoryczne
dummyModelTest <- dummyVars(Diabetes_binary ~ ., data = dane.test, fullRank = T)
dane.test.X <- as.data.frame(predict(dummyModelTest, dane.test))

# Przekształcamy zmienne numeryczne na zakres 0-1
rangeModelTest <- preProcess(dane.test.X, method = 'range')
dane.test.X <- predict(rangeModelTest, dane.test.X)

# Łączymy zmienną zależną i niezależne w zbiorze testowym
dane.test <- cbind(Diabetes_binary = dane.test$Diabetes_binary, dane.test.X)

# Sprawdzamy, czy nazwy poszczególnych kolumn w obu zbiorach się pokrywają, aby uniknąć późniejszych błędów
all(colnames(dane.train) == colnames(dane.test))
```

Przygotowanie modelu selekcji predyktorów.

```{r}
control <- rfeControl(functions = rfFuncs, # random forest funcs
                      method = "repeatedcv", # repeated cv
                      repeats = 5, # number of repeats
                      number = 10) # number of folds

x <- dane.train[-1]

y <- dane.train$Diabetes_binary

cl <- makeCluster(6, type = 'SOCK')

registerDoSNOW(cl)

start_rfe <- Sys.time()

rfe_result <- rfe(x = x, 
                   y = y, 
                   sizes = c(1:5, 10, 15),
                   rfeControl = control)

koniec_rfe <- Sys.time()

stopCluster(cl)
```

```{r}
# Print the results
result_rfe1
 
# Print the selected features
predictors(result_rfe1)
```

```{r}
#Zapis modelu do pliku
saveRDS(result_rfe1, "Modele/rfe_features_result.rds")
```

```{r}
rfe_result <- readRDS('Modele/rfe_features_result.rds')
```

Przygotowanie modelu regresji logistycznej

```{r}
zmienne <- c('Diabetes_binary', 'BMI', 'GenHlth.L', 'HighChol.1', 'HeartDiseaseorAttack.1', 'HighBP.1')
dane.train.glm <- dane.train[, zmienne]
dane.test.glm <- dane.test[, zmienne]
```

```{r}
fitControl <-  trainControl(method = "repeatedcv", 
                            number = 10, 
                            repeats = 20, 
                            summaryFunction = twoClassSummary,
                            classProbs = TRUE, 
                            savePredictions = T, 
                            sampling = 'down')

start <- Sys.time()

cl <- makeCluster(6, type = 'SOCK')
registerDoSNOW(cl)

glm_model <- train(Diabetes_binary ~ ., 
                   data = dane.train.glm, 
                   family = 'binomial', 
                   method = 'glm', 
                   trControl = fitControl)

stopCluster(cl)
koniec <- Sys.time()
```

```{r}
summary(glm_model)
```

```{r}
probs <- seq(.01, .99, by = .01)

start_ths <- Sys.time()

ths <- thresholder(glm_model,
                   threshold = probs,
                   final = TRUE,
                   statistics = "all")

koniec_ths <- Sys.time()
```

```{r}
glm_model <- readRDS('Modele/glm_reglog_model.rds')
```

```{r}
predykcja <- predict(glm_model, dane.test, type = 'prob')

predykcja <- ifelse(predykcja[,1] >= 0.5, 'no', 'yes') #0.5 wybieramy z thresholdera, balansujemy sensitivity i specificity

confusionMatrix(data = as.factor(predykcja),
                reference = dane.test$Diabetes_binary, 
                positive = 'yes')

conf_mat <- cvms::confusion_matrix(targets = dane.test$Diabetes_binary,
                                    predictions = as.factor(predykcja),
                                    positive = 'yes')

cvms::plot_confusion_matrix(conf_mat,
                            counts_on_top = T,
                            add_normalized = F,
                            rotate_y_text = F,
                            add_row_percentages = F,
                            font_counts = list(size = 10),
                            font_col_percentages = list(size = 8))
```

```{r}
#Zapis modelu do pliku
saveRDS(glm_model, 'Modele/glm_reglog_model.rds')
```
